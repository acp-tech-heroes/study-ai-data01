{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM2ku2TCJnv5zJtClnrDzlX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acp-tech-heroes/study-ai-data01/blob/main/%E7%AC%AC%E4%BA%8C%E5%9B%9EAi%E5%AD%A6%E7%BF%92%E4%BC%9Av1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 第二回Ai勉強会\n",
        "\n",
        "![タイトル](http://drive.google.com/uc?export=view&id=1Xjp7AonDpKigq7dreuvhVTyCPQ90BsYE)\n",
        "\n",
        "<hr/>\n",
        "\n",
        "## 前回のおさらい\n",
        "\n",
        "![前回のおさらい](https://drive.google.com/uc?export=view&id=1dgkdwz5-L1NhMkLgACW8JuwxGlfhiG_r)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![Python基本文法](https://drive.google.com/uc?export=view&id=1kVr-tfDFvfHmmEJ-di3oTv-493ppDqU_)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![Python Numpy](https://drive.google.com/uc?export=view&id=1NUCUxdD7fwiwf4cZDfhA7KTAuwHhrjn2)\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "![Aiの歴史](https://drive.google.com/uc?export=view&id=1r0LA5wAcL84j5kQ265BKo1GvLO7fLqyc)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![NNの基本構造](https://drive.google.com/uc?export=view&id=1WEd9yb5dzQqATOwSn8uAwCDxXI8gcKRp)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![BP:Step1](https://drive.google.com/uc?export=view&id=1x85KWs_84sCm80BH_vWDOPAhEACJJT4H)\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "![BP:Step2](https://drive.google.com/uc?export=view&id=1rd0BEz2byTHSZkenx5vX-l-r7pg9cZqj)\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "![BP:Step3](https://drive.google.com/uc?export=view&id=18kuv5Gm1T5Hw6UCDA9966egENIq8D0Ir)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![BP:Step4](https://drive.google.com/uc?export=view&id=1tqKOqtQwz3dfv7ErQhwCZGwBJ9YzfOVM )\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "![実際の学習工程](https://drive.google.com/uc?export=view&id=1n5zGSybJYETT_btvfQK9dI1V0U9obX2z)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![CNN](https://drive.google.com/uc?export=view&id=1P8xBYbEtsXyC-aUJ9z_TiEzFiidgK1Eu)\n",
        "\n"
      ],
      "metadata": {
        "id": "nR3KSJMniF9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![title1](https://drive.google.com/uc?export=view&id=11tUva1mAwPPQe49Egc9u9rGgBo4ZgRfB)\n",
        "\n"
      ],
      "metadata": {
        "id": "jfxF30Adkott"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![MLP](https://drive.google.com/uc?export=view&id=1rUr48lzbZePCPiWQ1DS_mT7oz0__tB9g)\n",
        "\n"
      ],
      "metadata": {
        "id": "XDceBD_VkswB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lTXC0zRgVDl",
        "outputId": "c6b82c19-c978-4a25-97bc-ecae7ddcacb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 250)               196250    \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 100)               25100     \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 222360 (868.59 KB)\n",
            "Trainable params: 222360 (868.59 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.2088 - accuracy: 0.9376\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0857 - accuracy: 0.9733\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0572 - accuracy: 0.9815\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0435 - accuracy: 0.9856\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.0346 - accuracy: 0.9888\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0288 - accuracy: 0.9905\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0242 - accuracy: 0.9920\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0220 - accuracy: 0.9933\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0180 - accuracy: 0.9940\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0176 - accuracy: 0.9940\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0896 - accuracy: 0.9803\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08963873982429504, 0.9803000092506409]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# データの準備\n",
        "# MNISTデータセットをロードし、訓練データとテストデータに分けます。\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# データを浮動小数点型に変換し、0から1の範囲に正規化します。\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# モデルの定義\n",
        "# Sequentialモデルを使用して、層を順に積み重ねていきます。\n",
        "model = keras.Sequential([\n",
        "  # Flatten層で28x28ピクセルの画像を784次元のベクトルに変換します。\n",
        "  layers.Flatten(input_shape=(28, 28)),\n",
        "  # 250ユニットの全結合層（Dense層）を追加し、活性化関数としてReLUを使用します。\n",
        "  layers.Dense(250, activation='relu'),\n",
        "  # 100ユニットの全結合層を追加し、活性化関数としてReLUを使用します。\n",
        "  layers.Dense(100, activation='relu'),\n",
        "  # 10ユニットの出力層を追加し、活性化関数としてsoftmaxを使用して多クラス分類を行います。\n",
        "  layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# モデルのコンパイル\n",
        "# モデルをコンパイルし、最適化アルゴリズムとしてAdamを、\n",
        "# 損失関数として交差エントロピー（sparse_categorical_crossentropy）を、\n",
        "# 評価指標として正答率（accuracy）を使用します。\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# モデルのサマリーを表示\n",
        "# モデルの構造を表示します。\n",
        "model.summary()\n",
        "\n",
        "# モデルの可視化\n",
        "# モデルの構造を図として保存し、層の形状と名前を表示します。\n",
        "plot_model(model, to_file='model_simple_mlp.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# モデルの学習\n",
        "# モデルを訓練データで学習させます。ここではエポック数を10に設定しています。\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "# モデルの評価\n",
        "# テストデータを使用してモデルを評価し、精度を確認します。\n",
        "model.evaluate(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![CNN](https://drive.google.com/uc?export=view&id=1gfn2wf0YQ1y4P99EBdyNM2bkzdcE4-J6)\n"
      ],
      "metadata": {
        "id": "tYQRxAmOkv28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras\n",
        "from keras.applications import VGG16\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# データの準備\n",
        "# MNISTデータセットをロードし、訓練データとテストデータに分けます。\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# データを浮動小数点型に変換し、0から1の範囲に正規化します。\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# モデルの定義\n",
        "# Sequentialモデルを使用して、畳み込み層や全結合層を順に積み重ねていきます。\n",
        "model = keras.Sequential([\n",
        "  # 32個のフィルタを持つ3x3の畳み込み層を追加します。入力形状は28x28ピクセルのグレースケール画像です。\n",
        "  layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  # 2x2のプーリング層を追加します。\n",
        "  layers.MaxPooling2D((2, 2)),\n",
        "  # さらに64個のフィルタを持つ3x3の畳み込み層を追加します。\n",
        "  layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "  # もう一つの2x2のプーリング層を追加します。\n",
        "  layers.MaxPooling2D((2, 2)),\n",
        "  # Flatten層で畳み込み層からの出力を1次元配列に変換します。\n",
        "  layers.Flatten(),\n",
        "  # 250ユニットの全結合層を追加します。\n",
        "  layers.Dense(250, activation='relu'),\n",
        "  # 100ユニットの全結合層を追加します。\n",
        "  layers.Dense(100, activation='relu'),\n",
        "  # 10ユニットの出力層を追加します。活性化関数としてsoftmaxを使用します。\n",
        "  layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# モデルのコンパイル\n",
        "# モデルをコンパイルし、最適化アルゴリズムとしてAdamを、\n",
        "# 損失関数として交差エントロピー（sparse_categorical_crossentropy）を、\n",
        "# 評価指標として正答率（accuracy）を使用します。\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# モデルのサマリーを表示\n",
        "# モデルの構造を表示します。\n",
        "model.summary()\n",
        "\n",
        "# モデルの可視化\n",
        "# モデルの構造を図として保存し、層の形状と名前を表示します。\n",
        "plot_model(model, to_file='model_cnn.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "# モデルの学習\n",
        "# モデルを訓練データで学習させます。ここではエポック数を10に設定しています。\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "# モデルの評価\n",
        "# テストデータを使用してモデルを評価し、精度を確認します。\n",
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "# モデルの保存\n",
        "# 学習済みのモデルをHDF5ファイルとして保存します。\n",
        "model.save('cnn_model.h5')\n",
        "\n"
      ],
      "metadata": {
        "id": "jFcQHhIfwLLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a55a12de-3efb-4717-e1b6-3addcf70b3a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 250)               400250    \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 100)               25100     \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 445176 (1.70 MB)\n",
            "Trainable params: 445176 (1.70 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 10s 4ms/step - loss: 0.1298 - accuracy: 0.9599\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0421 - accuracy: 0.9871\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0282 - accuracy: 0.9912\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0210 - accuracy: 0.9937\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0173 - accuracy: 0.9944\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0132 - accuracy: 0.9956\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0107 - accuracy: 0.9964\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0099 - accuracy: 0.9966\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0078 - accuracy: 0.9975\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0076 - accuracy: 0.9977\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0429 - accuracy: 0.9907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![Mobile&Image](https://drive.google.com/uc?export=view&id=1lTxht54AGG8GBIN0WAXz44EwzPJYk7K-)\n"
      ],
      "metadata": {
        "id": "YFY6Po2Qk2dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf study-ai-data01/\n",
        "!git clone https://github.com/acp-tech-heroes/study-ai-data01.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKzY7lA5ZecW",
        "outputId": "6ea11c82-5fda-4248-fda2-47f622a00b60"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'study-ai-data01'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects:   9% (1/11)\u001b[K\rremote: Counting objects:  18% (2/11)\u001b[K\rremote: Counting objects:  27% (3/11)\u001b[K\rremote: Counting objects:  36% (4/11)\u001b[K\rremote: Counting objects:  45% (5/11)\u001b[K\rremote: Counting objects:  54% (6/11)\u001b[K\rremote: Counting objects:  63% (7/11)\u001b[K\rremote: Counting objects:  72% (8/11)\u001b[K\rremote: Counting objects:  81% (9/11)\u001b[K\rremote: Counting objects:  90% (10/11)\u001b[K\rremote: Counting objects: 100% (11/11)\u001b[K\rremote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 82 (delta 0), reused 5 (delta 0), pack-reused 71\u001b[K\n",
            "Receiving objects: 100% (82/82), 87.29 MiB | 43.58 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import sys\n",
        "sys.path.append('./study-ai-data01')\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input, decode_predictions\n",
        "from ilsvrc2012_japanese import Ilsvrc2012Japanese\n",
        "\n",
        "# MobileNetモデルのロード（重みはImageNetを使用）\n",
        "model = MobileNet(weights='imagenet')\n",
        "\n",
        "# 画像ファイル名\n",
        "file_name = 'study-ai-data01/dog.jpg'\n",
        "\n",
        "# 画像の読み込みと前処理\n",
        "# 画像を読み込みます。target_sizeを(224, 224)に設定することで、\n",
        "# 画像がMobileNetモデルが要求する入力サイズにリサイズされます。\n",
        "img = image.load_img(file_name, target_size=(224, 224))\n",
        "\n",
        "# 読み込んだ画像をNumPy配列に変換します。\n",
        "# この操作により、画像は数値の配列に変換され、モデルによる処理が可能になります。\n",
        "img_array = image.img_to_array(img)\n",
        "\n",
        "# モデルに入力するために配列の次元を変更します。\n",
        "# np.expand_dimsは指定された軸に沿って新しい次元を追加します。\n",
        "# ここでは、バッチサイズの次元を追加しています（バッチサイズ=1）。\n",
        "# この操作により、画像配列がモデルが期待する形式になります。\n",
        "img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "# モデルに適した形で画像データを前処理します。\n",
        "# preprocess_inputは、MobileNetモデルに適した画像の前処理を行い、\n",
        "# ピクセル値のスケーリングなどを適切に調整します。\n",
        "img_array = preprocess_input(img_array)\n",
        "\n",
        "# 画像の分類\n",
        "predictions = model.predict(img_array)\n",
        "\n",
        "# 日本語変換クラスのインスタンス化\n",
        "converter = Ilsvrc2012Japanese()\n",
        "\n",
        "# 予測結果の英語ラベルを日本語に変換し、確率も表示\n",
        "for pred in decode_predictions(predictions, top=5)[0]:\n",
        "    english_label, probability = pred[1], pred[2]\n",
        "    japanese_label = converter.convert(english_label)\n",
        "    print(f\"Predicted: {english_label} ({japanese_label}), Probability: {probability:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MKIiJ1hiJAG",
        "outputId": "f7a73754-867e-4e42-9997-3a7da8cd60d1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 525ms/step\n",
            "Predicted: cocker_spaniel (イングリッシュ・コッカー・スパニエル（犬）), Probability: 0.57\n",
            "Predicted: Welsh_springer_spaniel (ウェルシュ・スプリンガー・スパニエル（犬）), Probability: 0.27\n",
            "Predicted: clumber (クランバー), Probability: 0.09\n",
            "Predicted: Sussex_spaniel (サセックス・スパニエル（犬）), Probability: 0.03\n",
            "Predicted: Irish_setter (犬（アイリッシュセッター）), Probability: 0.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![kyouka](https://drive.google.com/uc?export=view&id=117aDiHHNi2-xbGzipPQCrxsXHhvd26BN)\n",
        "\n"
      ],
      "metadata": {
        "id": "DGKkjO70k6JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# データセットのパス\n",
        "train_dir = 'study-ai-data01/janken-train'  # 訓練データのパスを設定\n",
        "validation_dir = 'study-ai-data01/janken-validate'  # テストデータのパスを設定\n",
        "\n",
        "# データ拡張\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# データの読み込み\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=9,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=9,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# MobileNetの読み込み\n",
        "base_model = MobileNet(weights='imagenet', include_top=False)\n",
        "\n",
        "# カスタム層の追加\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(3, activation='softmax')(x)  # 3クラス用の出力層\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# トップレイヤー以外のレイヤーを凍結\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# モデルのコンパイル\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# モデルの訓練\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLO-HKYqToq_",
        "outputId": "721570d5-320b-4e6a-bfb1-b404714850a2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 36 images belonging to 3 classes.\n",
            "Found 9 images belonging to 3 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4/4 [==============================] - 6s 1s/step - loss: 6.5917 - accuracy: 0.2500 - val_loss: 6.8568 - val_accuracy: 0.3333\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 3s 854ms/step - loss: 1.6315 - accuracy: 0.6389 - val_loss: 0.4584 - val_accuracy: 0.7778\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 4s 869ms/step - loss: 0.4135 - accuracy: 0.8056 - val_loss: 0.5038 - val_accuracy: 0.6667\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 3s 838ms/step - loss: 1.2915 - accuracy: 0.5833 - val_loss: 1.7328 - val_accuracy: 0.5556\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 4s 1s/step - loss: 1.1631 - accuracy: 0.5833 - val_loss: 0.4193 - val_accuracy: 0.7778\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 4s 846ms/step - loss: 0.2640 - accuracy: 0.8889 - val_loss: 0.7331 - val_accuracy: 0.7778\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 3s 849ms/step - loss: 0.2987 - accuracy: 0.8889 - val_loss: 0.3976 - val_accuracy: 0.8889\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 3s 814ms/step - loss: 0.4894 - accuracy: 0.7222 - val_loss: 1.2520 - val_accuracy: 0.6667\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.4857 - accuracy: 0.7778 - val_loss: 0.2773 - val_accuracy: 0.8889\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 3s 875ms/step - loss: 0.1112 - accuracy: 1.0000 - val_loss: 0.5086 - val_accuracy: 0.7778\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ed8a6108820>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![fine](https://drive.google.com/uc?export=view&id=13OX1Xo3dZva80QUWuNI4QACDGcxb1t4V)"
      ],
      "metadata": {
        "id": "osxoHX9kk-Py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# データセットのパス\n",
        "train_dir = 'study-ai-data01/janken-train'\n",
        "validation_dir = 'study-ai-data01/janken-validate'\n",
        "\n",
        "# MobileNetモデルの読み込み\n",
        "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# 上位層の凍結解除\n",
        "for layer in base_model.layers[:100]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[100:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# 新しいトップ層の追加\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# モデルのコンパイル\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 画像データの前処理\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical')\n",
        "\n",
        "# データジェネレータの定義（バッチサイズを考慮したsteps_per_epochの計算）\n",
        "batch_size = 32\n",
        "train_steps_per_epoch = np.ceil(train_generator.samples / batch_size)\n",
        "validation_steps = np.ceil(validation_generator.samples / batch_size)\n",
        "\n",
        "# モデルの訓練（steps_per_epochとvalidation_stepsを修正）\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mexwSWHY2aG8",
        "outputId": "f4cfc919-1017-4c18-cecb-aa4807fd7b08"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 36 images belonging to 3 classes.\n",
            "Found 9 images belonging to 3 classes.\n",
            "Epoch 1/10\n",
            "2/2 [==============================] - 7s 4s/step - loss: 1.4074 - accuracy: 0.3333 - val_loss: 1.2867 - val_accuracy: 0.5556\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 3s 783ms/step - loss: 1.0657 - accuracy: 0.6389 - val_loss: 1.4308 - val_accuracy: 0.5556\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 3s 3s/step - loss: 0.8041 - accuracy: 0.6944 - val_loss: 0.9507 - val_accuracy: 0.5556\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 4s 4s/step - loss: 0.4643 - accuracy: 0.8056 - val_loss: 0.7187 - val_accuracy: 0.6667\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 3s 801ms/step - loss: 0.5068 - accuracy: 0.7500 - val_loss: 0.5963 - val_accuracy: 0.7778\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 3s 3s/step - loss: 0.3426 - accuracy: 0.9167 - val_loss: 0.4812 - val_accuracy: 0.8889\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 4s 3s/step - loss: 0.3752 - accuracy: 0.8611 - val_loss: 0.5255 - val_accuracy: 0.8889\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 3s 780ms/step - loss: 0.2668 - accuracy: 0.9167 - val_loss: 0.6871 - val_accuracy: 0.7778\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 3s 796ms/step - loss: 0.2088 - accuracy: 0.9722 - val_loss: 0.8020 - val_accuracy: 0.6667\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 4s 4s/step - loss: 0.1872 - accuracy: 0.9444 - val_loss: 0.6349 - val_accuracy: 0.8889\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ed65574bdc0>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    }
  ]
}